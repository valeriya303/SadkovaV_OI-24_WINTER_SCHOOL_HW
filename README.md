
## Звіт про виконання фінального проєкту

### 1. Модернізація ігрового середовища (Environment Engineering)

Я взяла базовий прототип гри `Grid Coin Collector` і провела його глибоку архітектурну переробку для потреб Reinforcement Learning:

* **Створення абстракції середовища:** Перетворила гру з формату "Input-Output" (клавіатура-екран) у формат **MDP (Markov Decision Process)**. Це дозволило агенту взаємодіяти зі світом через стани та нагороди.
* **Оптимізація простору станів:** Розробила систему індексації координат  у лінійний простір (150 станів), що дозволило моделі ефективно використовувати пам'ять.
* **Дискретизація дій:** Чітко визначила простір дій (Action Space) для агента, що забезпечило стабільність навчання.

### 2. Prompt Engineering для дизайну економіки нагород

Одним із найскладніших етапів було налаштування нагород. Я використала **локальну LLM (Qwen/Llama)** через Ollama для генерації оптимальної стратегії винагород:

* **Впровадження "Time Penalty":** За порадою моделі було введено негативну нагороду () за кожен крок. Це змусило ШІ не просто блукати, а шукати найкоротший шлях до цілі.
* **Балансування штрафів:** Налаштувала штрафи за зіткнення зі стінами так, щоб агент уникав їх, але не боявся досліджувати карту.

### 3. Реалізація та тренування RL-агента

Я інтегрувала алгоритм **Q-Learning**, який дозволив агенту самостійно еволюціонувати:

* **Exploration vs Exploitation:** Налаштувала параметр  (epsilon) для балансу між вивченням нових шляхів та використанням вже знайдених маршрутів.
* **Аналіз прогресу:** Впровадила систему автоматичного логування та візуалізації метрик навчання. Створені графіки підтверджують, що з кожним епізодом середня нагорода зростає, а кількість помилок зменшується.

--- Чому це "класно і достойно"?

* **Sovereignty & Privacy:** Весь цикл — від генерації ідей через LLM до тренування агента — відбувався **локально** на моєму залізі без доступу до хмари.
* **Повний цикл розробки:** Я пройшла шлях від "Vibe Coding" гри до створення автономного інтелекту, що вміє в неї грати.
* **Масштабованість:** Створене мною середовище `grid_env.py` можна використовувати для тестування складніших алгоритмів, таких як Deep Q-Networks (DQN).

